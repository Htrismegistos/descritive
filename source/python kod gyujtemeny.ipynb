{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new findings\n",
    "#bins in df\n",
    "pd.cut\n",
    "#filter isnull() return\n",
    ".loc[isnull().to_frame() != 0]\n",
    "\n",
    "#comprehansion\n",
    "dic = {a:x for a,x in zip(listA,listx)}\n",
    "\n",
    "#generator function, not created\n",
    "def gen(n =100)\n",
    "    i = 0\n",
    "    while i< n;\n",
    "        yield i*n\n",
    "        i+=1\n",
    "print(list(gen())[:10]) #to call needs to put to frame. \n",
    "\n",
    "#messege generaratio\n",
    "raise ValueError('messege')\n",
    "\n",
    "# Assert that all values are >= 0\n",
    "assert (df >= 0).all().all() #return nothing if true. turns error if not true. all().all() used because of multiple cols\n",
    "\n",
    "#matplotlib geomaps\n",
    "folium, basemap\n",
    "\n",
    "#converge rows to cols\n",
    "pd.melt(frame=df, id_vars = , value_vars = ['',''], value_name='for the data', var_name = 'joint colnnames')\n",
    "#separate mixed feature columns reshape the df without agg. cat dtype col. will be the columns,  \n",
    "pd.pivot(index='record_id', value='value col', columns='col of the mixed feature names')\n",
    "\n",
    "##add new col to 2D\n",
    "np.append(old, new, axis = 1) #append new col\n",
    "\n",
    "# np matrix felosztas listaba - fisrt converts to dates. this dates linux dates, works only with plt.plot_dates()\n",
    "## this is the pattern of the original numbers\n",
    "import matplotlib.dates as mdates\n",
    "x, y, z, np.loadtxt(arr, unpack = True, delimiter = ',', converteds = {0: mdates.bytespdate2num('%y%m%D')})\n",
    "\n",
    "# custom legedns\n",
    "import matplotlib.patches as mpatches\n",
    "red_patch = mpatches.Patch(color='Green', label='Remain',)\n",
    "g_patch = mpatches.Patch(color='Red', label = 'Churn')\n",
    "plt.legend(handles=[red_patch, g_patch])\n",
    "\n",
    "#get the levels into list in original order, originally listed alphabetically not in df order.\n",
    "#useful at plot labels\n",
    "df.index.get_level_values(1) # level1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting filenames from directory\n",
    "for i in os.listdir(os.getcwd()):\n",
    "    if i.startswith('') & i.endswith():\n",
    "    if os.stat('file').st_size > 0 # check filesize\n",
    "\n",
    "glob.glob(pattern) #slower then os.listdir\n",
    "\n",
    "#list of created variables\n",
    "dirs()\n",
    "\n",
    "#empty variable\n",
    "var = None\n",
    "var = \"\"\n",
    "#check if var exist as local or global var\n",
    "if 'var' in locals() globals()\n",
    "\n",
    "#generate variables on the fly ()\n",
    "for i in uni.keys():\n",
    "    vars()[i]=countdick(data[1],i)\n",
    "    print(vars()[i])\n",
    "\n",
    "#generator function, not created, no memory allocation\n",
    "def gen(n =100)\n",
    "    i = 0\n",
    "    while i< n;\n",
    "        yield i*n\n",
    "        i+=1\n",
    "\n",
    "#loop conditionlly\n",
    "while True:\n",
    "    if data == 'no':\n",
    "        break\n",
    "    data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shorthands\n",
    "    # lambda to define function \n",
    "lambda var : var*2\n",
    "\n",
    "    # map for 'for' loops , instantly combine with lambda shorthand function an a given element\n",
    "map(what_to_do, on_what_list_element)\n",
    "map(lambda x: x*2, list1) # takes elements one by one and runs the lambda function on them\n",
    "map(func(), list1)\n",
    "\n",
    "    # filter for if statement\n",
    "list(filter(lambda x: X%2 == 0, list1)) # returns a list with only the even numbers in the list1\n",
    "\n",
    "#dict comprehansion\n",
    "dic = {a:x for a,x in zip(listA,listx)}\n",
    "gen = (n for i in b) #repeats n as long a b iterates #not created, doesnt take memory. useful if hefty big can iterate\n",
    "print(next(gen))\n",
    "\n",
    "\n",
    "list(filter(lambda x: X%2 == 0, list1)) # returns a list with only the even numbers in the list1, list comprehension 3x faster\n",
    "\n",
    "###################\n",
    "'''\n",
    "    #list comprehension\n",
    "NetProfit = [int(i*0.7) for i in Profit]\n",
    "Profit = [rev[i] - exp[i] for i in range (len(rev))]\n",
    "checks = [ i is not None and i >30 for i in values]\n",
    "obj = [\"Even\" if i%2==0 else \"Odd\" for i in range(10)]\n",
    "file1 = [ i for i in file if len(i) > 1] #ha nincs else az if a vegere jon\n",
    "names=[k for k,v in name_counts.items() if v>=2]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get url and put in variable without saving\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "with urlopen(Request('http://www.datacamp.com/teach/documentation')) as r:\n",
    "    r = BeautifulSoup(r.read()).body #if html\n",
    "    a = [i.get('href') for i in r.find_all('a') if i.get('href') is not None if i.get('href')[:7] == 'http://']\n",
    "    #or if file with chunk read\n",
    "    f = [r.readline() for i in range(10)]\n",
    "    #or json\n",
    "    j = json.load(r)\n",
    "\n",
    "#get file from url and save as file\n",
    "import urllib.reqest import urlretrieve\n",
    "urlretrieve(url,'filename.csv')\n",
    "\n",
    "#simple open need to close. this clause close it natively\n",
    "with open(file) as f:   #for binary file (pickle) use 'rb'\n",
    "    data = [f.readline() if i >500 and i < 1000 else f.readline() == '' for i in range(2000)] #load 2nd chunk 500 rows \n",
    "    #or\n",
    "    print(f.readline()) #single line readout\n",
    "    #or\n",
    "    d = pickle.load(f)\n",
    "\n",
    "#data from API request is better then urllib: has SSL verificatin, handles full API support\n",
    "import requests\n",
    "js = requests.get('to_api', verify  = False).json()\n",
    "js.keys()\n",
    "\n",
    "\n",
    "#supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import primitive way -- bad handling of txt separator\n",
    "f=[i.split(',') for i in open('').read().split('\\n') if len(i.split(',')) >1 ] #sensitive for ',' in text\n",
    "#import csv -- prefered\n",
    "import csv\n",
    "f=[i for i in csv.reader(open('', 'r')) if len(i) > 1]\n",
    "\n",
    "#import to np\n",
    "import numpy as np\n",
    "f=np.genfromtxt('xxx.csv',dtype='U75', skip_header=1, delimiter=',')\n",
    "#dtype=u75 unicode, nelkule float-kent akarja, igy a szoveges reszek is a helyukon maradnak\n",
    "\n",
    "#import panda\n",
    "import pandas as pd\n",
    "f=pd.read_csv('', na_values={'oszlop': [list of phrases to convert]}, chunksize= int) #chunksize iterate over the file\n",
    "next(f) #takes the next chunk\n",
    "\n",
    "#import from excel\n",
    "f= pd.ExcelFile(.xls)\n",
    "print(f.sheet_names)\n",
    "f=pd.read_excel('filename','sheetname1')\n",
    " #or\n",
    "xls = pd.read_excel(url) #dictionay of df-s\n",
    "xls.keys()\n",
    "xls['sheetname'].head()\n",
    "\n",
    "#HDF5 dictionary in dictionary\n",
    "import h5py\n",
    "f = h5py.File('.hdf5', 'r')\n",
    "f.keys\n",
    "\n",
    "%timeit #meri a futasi idot\n",
    "\n",
    "from pandas_datareader import data, wb #import stock data from internet https://pandas-datareader.readthedocs.io/en/latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#failure avoidance\n",
    "if x < 0:\n",
    "    raise ValueError('messege') #kondicionalis uzenetkuldes vegrehajtas nelkul\n",
    "try:\n",
    "    expression\n",
    "except KeyError as e: # kiirja mi volt a hiba, TypeError, ValueError\n",
    "    print(type(e))\n",
    "    print(e)\n",
    "    pass \n",
    "#other solution\n",
    "    expession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create random sample from data with numeric index\n",
    "##create random index sample and filter with that\n",
    "from random import sample\n",
    "df.iloc[sample(range(len(df), sample_size))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text manipulatin\n",
    "\n",
    "#df, string : remove left, right and all the empty\n",
    "data['result'] = data['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC').replace(\" \",\"\"))\n",
    "x.strip(' \\t\\n\\r') #only from left and right\n",
    "re.sub('[\\s+]', '', x)\n",
    "title(r\"$\\bf{text}$\") #bold\n",
    "\n",
    "#print only certain file type\n",
    "for i in os.listdir('/run/..'):\n",
    "    if '.csv' in i:\n",
    "        print(i)\n",
    "        \n",
    "# wildcards\n",
    "https://www.python-course.eu/python3_re.php\n",
    "\n",
    "#####**finding a pattern**\n",
    "l = 'ttt dog ttt dag'\n",
    "print(l.find('dog')) #first pos\n",
    "print(l.__contains__('dog')) #True\n",
    "print('dog' in l) #True\n",
    "print(re.search('dog', l, re.I).span()) # only finds the first return tuples of start/stop pos\n",
    "print(re.match('dog', l)) #checks only at the begining\n",
    "re.findall('d.{2}',l) # list, returns the found search patterns [dog,dag]\n",
    "\n",
    "#regular expression\n",
    "pattern = re.compile('\\$\\d*.\\d{2}$') #creates expr. \\$ real $ char, any decimal, 1 any  char, 2 decimal, finish here\n",
    "result = pattern.match(query) #a query szovegben keresi a pattern-t\n",
    "\n",
    "#filter out\n",
    "df.loc[df[col].map(lambda x: '?' in str(x))] \n",
    "# for complex search re.search -> boolean if word in sentence add to list\n",
    "l = [text for text in list_of_sentences if re.search(word, text)]\n",
    "df['title'].map(lambda x: re.match(r'.*chief.*', x, re.I)).count() # re.I ignore case \n",
    "#check all cols returns summary\n",
    "df.apply(lambda x: str(x).find('?')) \n",
    "# contian works only in df series\n",
    "df[df['title'].str.lower().str.contains(r'[cf]ig')]['Id'].count() # erdemes lower-case formalni az esetleges elutesek miatt\n",
    "\n",
    "\n",
    "#greek letters\n",
    "r'$\\alpha$', r'$\\beta$', r'$\\gamma$', r'$\\delta$', r'$\\epsilon$'\n",
    "p = '^.\\d{2}' #start 1 any char, two decimal\n",
    "p = '.*.csv$' #returns everything anding with .csv definitive end match\n",
    "p = '.{2}.csv' #returns 2 chars,matching '.csv', ignores rest at begining and end\n",
    "p = '\\d+' # returns any lenght numbers\n",
    "p = '\\d.' # same\n",
    ". # any char\n",
    "\\. # true dot\n",
    ".* # 0 - indefinite repetition of any char\n",
    "+ # 1- infinite repetition\n",
    "'{4,5}' # given number of repetition : 4 or 5 times\n",
    "'e?' # possible character or position\n",
    "[^c] # negates c\n",
    "\n",
    "[ab3d-f] # any from the list on the position, from d to f in alphabet\n",
    "\\d #Matches any decimal digit; equivalent to the set [0-9].\n",
    "\\D #The complement of \\d. It matches any non-digit character; equivalent to the set [^0-9].\n",
    "\\s #Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v].\n",
    "\\S #The complement of \\s. It matches any non-whitespace character; equiv. to [^ \\t\\n\\r\\f\\v].\n",
    "\\w #Matches any alphanumeric character; equivalent to [a-zA-Z0-9_]. With LOCALE, it will match the set [a-zA-Z0-9_] plus characters defined as letters for the current locale.\n",
    "\\W #Matches the complement of \\w.\n",
    "\\b #Matches the empty string, but only at the start or end of a word.\n",
    "\\B #Matches the empty string, but not at the start or end of a word.\n",
    "\\\\ #Matches a literal backslash.\n",
    "'^b', # mark the beginning of a string start with b\n",
    "'b$', # mark the end of a string, finish with b\n",
    "r\"Feb(ruary)? 2011\" #subexpression Feb 2011 and February 2011\n",
    "\n",
    "## formating print\n",
    "class style: #linux format codes\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "print(style.BOLD+text+style.END)\n",
    "print('\\033[1m{}\\033[0m {}'.format(text, other))\n",
    "\n",
    "#####################\n",
    "#####################\n",
    "'''\n",
    "print('fff {0}, fff {1:,.2f}, f {0:.1f}'.format(0=X,1=y)) #print format nameing the position, ',' put ',' after 3 digits\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list tricks\n",
    "\n",
    "ll = set([1,2,3,1,2,1,2,3,3,3,3,2,2,2,1,1,2]) # gives back unique elements\n",
    "set.add(5) \n",
    "\n",
    "#list append\n",
    "w, t = [],[]\n",
    "l = [w,t] vs. [*w,*t] # nested list, appended list elements taken individually\n",
    "\n",
    "\n",
    "######################\n",
    "######################\n",
    "'''\n",
    "#find in list: good for cicles\n",
    "find = 'what' in lists #returns boolean\n",
    "drop = ll.pop(1) # drop the indexed item from the list and assign to var\n",
    "\n",
    "#index/ertek meghivasa. tuple-t ad vissza\n",
    "for index,value in enumerate(lista):\n",
    "    if value in lista2:\n",
    "        print(lista2[index])\n",
    "        print ('vv %.1f%%' % percent)\n",
    "\n",
    "#string positions indexed \n",
    "    w = world\n",
    "    w[0] => 'w'\n",
    "ll = sorted(lists)\n",
    "l.index('what') # returns the pos of 'what'\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np tricks\n",
    "\n",
    "# data generation\n",
    "np.random.randint(low=0, high=10, size=(5, 2)) # random szamsor\n",
    "np.random.rand(1,2) / .randn() # random between 0-1 / between -1 : 1\n",
    "np.random.chose(['a','b']) #for cat vars\n",
    "np.random.poisson(lam = rd.randrange(10,1000), size = 5)\n",
    "np.repeat([1,2,3,4], n) # repear the sequence n times\n",
    "np.zeros(), np.ones, np.linspace('evenly distrbuted'), np.arange, np.eye('symetric 2d array with 1 in the middle')\n",
    "np.arange(2,10,2) #szamsor generalas betweem 2-9, by 2\n",
    "#\n",
    "\n",
    "np.flipud(np.unique(X{:,1})) # array sorrend forditas, ha 1D, array egyedi ertek kiadas\n",
    "text, count = np.unique(X[:,3], return_counts= True) #two arrays\n",
    "arr[(arr>5) & (arr < 10)] # conditional selection returns an array slice\n",
    "f = np.hstack((file1,np.ones(len(file1)).reshape(-1,1))) #add new col of ones\n",
    "file2 = np.delete(file2, 4, axis = 1) #5th cols del\n",
    "ps[ps[:,1].argsort()] #sort by the given col\n",
    "\n",
    "# martix generalas listabol\n",
    "list1 = []\n",
    "list2 = []\n",
    "list3 = np.arange(2,10,2) #szamsor generalas\n",
    "arr = np.array([list1,list2,list3])\n",
    "list3.reshape(2,2)\n",
    "np.matrix.round(list3)/...\n",
    "martix[row0:row2,col0:col2] # slicing matrix\n",
    "#empty slot filled with NaN then convert to float. slicing doesnt truncate the array\n",
    "    X=np.array(f)\n",
    "    X[X == ''] = 'NaN'\n",
    "    X[:, 1:3].astype(float)\n",
    "#ha sorban van nem oszlopban, pl. simple regresszional \n",
    "    X.reshape(1,len(X)) #ha nem kepes atformazi egy lepesben\n",
    "    X = X.reshape(-1,1)\n",
    "    \n",
    "# np matrix felosztas listaba - fisrt converts to int, second to float\n",
    "import matplotlib.dates as mdates\n",
    "x, y, z, np.loadtxt(arr, unpack = True, delimiter = ',', converteds = {0: int, 1: float})\n",
    "\n",
    "#####################\n",
    "#####################\n",
    "'''\n",
    "np.append(old, new, axis = 1) #append new col\n",
    "np.around('list', 1) #kerekites 1 tizedesre. array-t ad vissza\n",
    "vector.astype(float) # atalakitja az adattypust. NaN ha nem tudja\n",
    "w = vector.sum(axis = 0) /mean()/... # soronkent\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary tricks\n",
    "dick = dict(zip(list,list))\n",
    "dick ={a,b for a,b in zip(lista,listb)}\n",
    "dictionary[new_key] = dictionary.pop(old_key) #atnevezes\n",
    "dick2.update(dick1) # merge dicts\n",
    "\n",
    "#add new value to dictionary if value in list, actually append to the list and not to the dick\n",
    "dick['key'] = []\n",
    "dick['key'].append('new')\n",
    "#or make it a list\n",
    "dick['key'] = [dick['key'], new]\n",
    "    \n",
    "#unique value counting to dictionary\n",
    "#if you dont use 'else:' first assigment have to be dick[i]=0\n",
    "#otherwise add plus one to every value\n",
    "#using 'else:' assigment is dick[i]=1\n",
    "\n",
    "#recursive dick to recursive df\n",
    "vars()['df'+str(n)]=pd.DataFrame() # create the df with idential name\n",
    "for k in data.keys(): #populate the df with the dick\n",
    "    vars()['df'+str(n)][k]=data[k][:len(data['id'])]\n",
    "    \n",
    "####################\n",
    "####################\n",
    "'''dick.keys()\n",
    "dick.values()\n",
    "del dick[oszlop]\n",
    "dick.pop(oszlop) #erase\n",
    "\n",
    "#delete multiple keys\n",
    "dels = ['key1','key2']\n",
    "for i in dels:\n",
    "    del dic[i]\n",
    "\n",
    "#key/value kapcsolat\n",
    "for k,v in enumerate(dick.items()):\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "#multiple return ex. in if statement add retrun to everywhere\n",
    "if f in d:\n",
    "    return f\n",
    "else: return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv-bol nested lista kezelesre\n",
    "for i in listoflists:\n",
    "    if i[4] is None:\n",
    "        i[4] = 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store data in dictionary\n",
    "from pandas.api.types import is_numeric_dtype #szam tipus teszteles\n",
    "dic = {}\n",
    "for i in d_csv.columns:\n",
    "    if is_numeric_dtype(d_csv[i]) #csak a szam alapu oszlopokkal dolgozik\n",
    "        dic.update({i: {'mean': d_csv.mean(),'median': d_csv.meadian()}})\n",
    "\n",
    "\n",
    "##########################\n",
    "##########################\n",
    "'''#using keys\n",
    "v0 = list(dic.keys()) #listthem in list\n",
    "print('Stat: '+', '.join(v0)) #print them without bracket concat with txt\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all').transpose() # a soveges oszloprol is ad infot\n",
    "df.count()\n",
    "\n",
    "df=df.iloc[::-1]  #reverse order\n",
    "\n",
    "# change index\n",
    "df=df.set_index('col') # origina data order but diferent index\n",
    "df=df.reindex(np.random.permutation(df.index)) #reshuffles the data. if index previously not listed give NaN, if index missing drops data\n",
    "df=df.reindex(index=df.index[::-1]) #reverse order, it rearrange the index, not change\n",
    "#revert to default\n",
    "df = df.reset_index(drop=True) #drops old index, otherwise keeps in new column\n",
    "\n",
    "df[df['col'].str.contains(r'An' flags=re.I, regex = True)][['col1','col2']]\n",
    "df[df['A'].str.contains(\"Hello|Britain\")==True]# reszleges szoveg egyezes\n",
    "df[~df.col.str.contains('xy')] # filter out '~': opposite\n",
    "df.filter(\"Hello|Britain\", axis = 0)\n",
    "df.pct_change() # percent change compared to previous row 50x gyorsabb mint a while loop\n",
    "\n",
    "# percent change compared to previous row slow but can chage the calculation\n",
    "i = 0\n",
    "profit = pd.DataFrame(columns = r.columns.levels[0])\n",
    "while i < len(r)-1:\n",
    "    n = i+1\n",
    "    p = (r[n:n+1].values / r[i:n].values)-1 # calculation  \n",
    "    profit.loc[r.index[n]] = p[0]\n",
    "    i+= 1\n",
    "    \n",
    "#search for min value in each cols and return index .argmin()\n",
    "profit.apply(lambda x: x.argmin(), axis = 0)\n",
    "profit.idxmin() #4x gyorsabb\n",
    "\n",
    "#using custom functions over the df \n",
    "any_function(row, arg)\n",
    "    row['cols'] == arg\n",
    "df = df.apply(any_function, axis=1, arg = arg) #function without (), funct arg added\n",
    "df.applymap() #takes elements but goes around the cols\n",
    "df['oszlop'].map() #only the give col, elementwise\n",
    "\n",
    "import numpy as np\n",
    "df.replace('mit', np.NaN) #karakter lecserelese NaN ertekre\n",
    "\n",
    "###################\n",
    "###################\n",
    "'''\n",
    "#slicing\n",
    "col_names = df.columns.tolist() #oszlopnevek listaba\n",
    "slicing = col_names[:2] + col_names[-2:] #slicing + concatenate\n",
    "df[slicing].head(2)\n",
    "df.loc[:,'oszlo':'oszlop':2] #oszlop alice, minden masodik\n",
    "df.loc[df[oszlop].isin([list])] #filter by values in a list\n",
    "df[slicing[:2]][2:20:3] # oszlop slice + sor slice, minden 3.\n",
    "df.iloc[:, 1:4] #slice by col rank\n",
    "df.iloc[:, 1::3] #slice by col rank, every 3rd\n",
    "df.iloc[:, [1,3,6]] #slice by col rank\n",
    "df[::10] #every 10th row\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#panda tricks\n",
    "#df from lists\n",
    "pd.Dataframe({'oszlop' : lista, 'oszlop2': lista2})\n",
    "pd.DataFrame(columns=[*w,*t], index=s) #empty df, cols from 2 lists, listed after each others. without *, multilevel\n",
    "d = zip([col_nam, [data]],[col_name2, [data2]])\n",
    "pd.DataFrame(d)\n",
    "##generate 2D data matrix (lis of lists or np) and a colname list --- absolute unnecessary compliction\n",
    "def listd(list1, list2):\n",
    "    ll = {l1:l2 for l1,l2 in zip(list1,list2)}\n",
    "    return ll\n",
    "diclist = [listd(colname,datarow) for datarow in _2Ddatamatrix]\n",
    "df= pd.DataFrame(diclist)\n",
    "# or\n",
    "df = pd.DataFrame(np, cols=colname )\n",
    "\n",
    "\n",
    "# belso calculation and correlation\n",
    "df[['len','Total']].corr()\n",
    "    \n",
    "#Series sort\n",
    "S.sort() #onmagaba ir vissza, nem kell valtozoba rakni kombinalva df.unique()\n",
    "\n",
    "#select specified datatypes in df\n",
    "df.select_dtypes(include='category').columns\n",
    "l = [i for i in df.columns if pd.api.types.is_numeric_dtype(df[i])]\n",
    "\n",
    "#calculate ddifferences between rows\n",
    "df.diff()\n",
    "\n",
    "#calculate between two cols by rows:\n",
    "crss2 = crss.apply(lambda x: x[0]/x[1], axis = 1) \n",
    "\n",
    "#############################\n",
    "#############################\n",
    "'''#sort, replacing the old one instead of creating a new\n",
    "df.sort_values(\"oszlop\", inplace=True, ascending=False)\n",
    "df = df[[oszlop2,oszlop4]] # oszlop sorrend atrendezes\n",
    "\n",
    "# belso calculation\n",
    "df['len'] = df['title'].apply(len)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful financial methods\n",
    "df.idxmin() / idxmax() # returns index for the min/max value for each cols\n",
    "df.resample('D').sum() \n",
    "df['bank'].xs['D1':'D2'].rolling(window=30).mean().plot() # 30 day rolling avr. in plot\n",
    "df.pct_change() # percent change compared to previous row 50x gyorsabb mint a while loop\n",
    "# compares the closing prices together to which currencies goes together.  \n",
    "sns.clustermap(df.xs('Close', axis = 1, level = 'forex').corr(), annote = True) \n",
    "\n",
    "# show loss or gain\n",
    "plt.fill_between(time, closep,\n",
    "                close[0], # the price of reference, buying price eg. exact No. also good\n",
    "                where = (closep > close[0]), #shows only when higher then the buying price\n",
    "                facecolor = 'g',\n",
    "                alpha = 0.3\n",
    "                ) # another can be made for loss with different facecolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c4a38769dfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#multiindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#https://pandas.pydata.org/pandas-docs/stable/advanced.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# renameing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#multiindex\n",
    "#https://pandas.pydata.org/pandas-docs/stable/advanced.html\n",
    "\n",
    "#creating multiindex\n",
    "ind = [df['col']. values, df['col2'].values]\n",
    "ind.columns = ind.columns.remove_unused_levels() #remove residual enrties\n",
    "indM = pd.MultiIndex.from_array(ind, names = ['col','col2'])\n",
    "ind.set_index(indM, drop=True, inplace=True)\n",
    "#creating multiindex from normal df\n",
    "df.groupby(['col1','col2']).count() #col1 level0, col2 level1 index\n",
    "\n",
    "\n",
    "#fill up the empty labels\n",
    "lab1 = list(ind.index.label[1])\n",
    "for i, v in enumerate(lab1):\n",
    "    if v < 0 :\n",
    "        lab1[i] = lab1[i-1]\n",
    "\n",
    "df.drop([('0col1', '1col1'),('0col2', '1col2')], axis = 1) # list of tuples for the columns to delete\n",
    "df.index.names ; levels ; labels\n",
    "df.index.set_names(['',''], level = 0, inplace= True)\n",
    "df.columns.set_levels(['',''], level = 1, inplace=True) # renameing\n",
    "df.sort_values([('Group1', 'C')], ascending=False) #group1 level0, c level1\n",
    "#rearrangin columns\n",
    "    #get col list\n",
    "df.columns\n",
    "    #rearranging/renaming cols\n",
    "    # LEVEL0 lista, level1 lista,, mennyi oszlopot foglal el a level0 (a szam a level0 lista pozicio, \n",
    "    #a sorrendiseg a megjelenes sorrendje - igy lehet atnevezni az oszlopfejlecet), az alaposzlopok a level2 \n",
    "    #elnevezessel(minden oszlopra meg kell adni a sorrendiseget a level1 lista sorrnd alapjan, de kulonbozo \n",
    "    #permutacok lehetsegesek)\n",
    "    # itt az utolso fooszlop (0col2) csak egy aloszlopot kap (1col1)\n",
    "colarray = [[['0col2', '0col0','0col1'], ['1col0','1col1']], [[1,1,2,2,0,0],[0,1,0,1,1]]] \n",
    "multicol = pd.MultiIndex.from_array(colarray)\n",
    "multicol = pd.MultiIndex(labels = [['0col2', '0col0','0col1'], ['1col0','1col1']], level = [[1,1,2,2,0,0],[0,1,0,1,1]])\n",
    "df = pd.DataFrame(data, columns=multicol, index = multiind)\n",
    "#slice\n",
    "df[slice(None),'1ind2'] # masodik index szint masodik eleme az osszes elso szintbol\n",
    "dl.loc[1:5,('0col2')] #teljes masodik oszlopcsoport\n",
    "df.loc[1:2,(slice(None),['y_pol1','y_pol2'])] #sor 1,2 , fo szint mind, alszint ket elem\n",
    "df['0col2','1col2'] = 12 #data add even if column doesn't exist yet\n",
    "df.loc[df['0col1','1col0'] == 'something']\n",
    "df.loc['level0value'].loc['level1value']\n",
    "df.xs(['level0value','level1value'])\n",
    "df.xs(['level1value'], level = 'Level1IndexName')\n",
    "# slice with IndexSlice\n",
    "idx = pd.IndexSlice\n",
    "ind = df[idx[['Datum' ,'Termék','Ki', 'TESCO']]] #mind level0, ha csak egy szint\n",
    "df.loc[idx[index], idx[[col_lev0], [col_lev1]]] # tobb szntnel kell a loc es az index is\n",
    "df.loc[idx[:, :, ['C1', 'C3']], idx[:, 'col_lev1']] # tobb szntnel kell a loc es az index is\n",
    "ind = df.loc[idx[:, 'level1'], idx[['Datum' ,'Termék','Ki', 'TESCO'], [:, 'oszlop']]] #index level1 es col\n",
    "df.set_value(idx['ind0', 'ind1','ind2'],idx['col0','col1'], 'value') # change value at given position\n",
    "\n",
    "df.sort_index(ascending= True, level=0, axis =1, inplace = True)\n",
    "#sort by value inside group\n",
    "df.sort_values(ascending= False).sort_index(level=[0], sort_remaining=False)\n",
    "#get the fisrt row of every group of sorted values\n",
    "df.groupby(level=0).head(1)\n",
    "#get the levels into list in original order, originally listed alphabetically not in df order.\n",
    "#useful at plot labels\n",
    "df.index.get_level_values(1) # level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby calc\n",
    "#pivot separates one col(value) by two cols(index,col), \n",
    "#groupby creates multiindex of the features, and can do several cols as values\n",
    "\n",
    "#convert cols to rows : organise same features to one col\n",
    "df = pd.melt(frame=df, id_vars = , value_vars = ['',''], value_name='for the data', var_name = 'joint colnnames')\n",
    "#separate mixed feature columns. ony even no of measures\n",
    "pd.pivot(index='record_id', values='value col', columns='col of the mixed feature names')\n",
    "#separate mixed feature columns uneven measures need to be aggragated\n",
    "v = df.pivot_table(index=\"oszlop_by\", values='value col', columns='col of the mixed feature names' aggfunc=np.mean)\n",
    "#index: categories, values categories. to count them needs to transform them on the fly with map\n",
    "df.pivot_table(index = ['Credit_History'],aggfunc= lambda x: x.map({'Y':1, 'N':0}).sum(), values='Loan_Status')\n",
    "#grupby 3x faster\n",
    "df.groupby('Credit_History').agg({'Loan_Status': lambda x: x.map({'Y':1,'N':0}).sum()})\n",
    "## can multiindex if\n",
    "index=['oszlop1','oszlp2']#Multigroup pivoting, hierathical indexing\n",
    "\n",
    "\n",
    "v = df.groupby('oszlop').sum() / mean() etc\n",
    "v.query('oszlop2'==['key']) #szutes\n",
    "v.loc[(key1,key2),'oszlop'] #a cella erteket adja vissz\n",
    "\n",
    "df.groupby('DONOR_AGE').size() # az adott oszlop kulonbozo ertekeinek elofordulasa\n",
    "df['DONOR_AGE'].value_counts() # az adott oszlop kulonbozo ertekeinek elofordulasa\n",
    "df.groupby('Age')['Exit'].agg(['sum','count']) #az exit ertekeket sum es count formaban is visszaadja\n",
    "df.groupby('Age').agg({'Exit': ['sum','count']}) #ld fent, de itt tobb oszlop sorsat is meg lehet adni\n",
    "\n",
    "#get the fisrt row of every group of sorted values\n",
    "df.groupby(level=0).head(1)\n",
    "## sort the values in the groups\n",
    "df.groupby('Age')['Val'].sum().sort_values(ascending=True).sort_index(level=[0], sort_remaining =False).groupby(level=0).head(1)\n",
    "\n",
    "\n",
    "http://pbpython.com/pandas-pivot-table-explained.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/advanced.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DataFrame manipuation\n",
    "\n",
    "df = pd.DataFrame(data=,columns=,index=datum)\n",
    "df.index = datum #index megvaltoztatasa\n",
    "df.dtypes\n",
    "del df['oszlop']\n",
    "osszevon = pd.concat([df1, dfx], axis = 1) #ures 'NA' kitoltes, oszlopokat rakja egymas melle.\n",
    "osszevon = pd.concat([df1, dfx]) #egymas ala rakja\n",
    "pd.merge(df1,df2, left_index=True, right_index=True, how='inner') #based on common cols, sql join\n",
    "df = df.append('df2') # a 'df2'-ot egysegkent adja hozza, alafuzi\n",
    "df.extend('df2') # a df2-t betunkent/elemenkent adja hozza\n",
    "df.drop('oszlop', axis = 1) #['oszlop1', 'oszlop2']\n",
    "pd.to_numeric(df['oszlop'], errors='coerce') #not changable value turns to NAN\n",
    "df.ix[1:4, 'col'] # combine iloc for index, loc for cols\n",
    "df.at[5,'col'] #matrix tipusi cella ertek kiiras index/oszlopnev alapjan\n",
    "df.iat[4,5] #sor/oszlop leszamolasaval adja a cellaerteket\n",
    "df.loc[(df['col1'] <= 10) & (df['col2'].str.contains('A'))]\n",
    "df.loc[indexertek, oszlop] #az adott sor oszlop ertekeknek kiirasa\n",
    "df['oszlop'][index] # kiirja a erteket\n",
    "df.loc[df['H'].notnull()] # .isnull() sorok melyek nem uresek/uresek\n",
    "row_index_83_in_age = df.loc[83,\"age\"]\n",
    "row_index_766_in_pclass = df.loc[766,\"pclass\"]\n",
    "df=df.[oszlop].astype('category')\n",
    "df[oszlop].cat.categories #listaba rakja a categoria ertekeket\n",
    "#new col for the occurnce percent\n",
    "df[ujoszlop] = us[oszlop].apply(lambda x: round((x/np.sum(us[oszlop])*100), 2))\n",
    "df.applymap(lambda x: x*2, axis =1) # takes the row and applies \n",
    "#create bins fro mnumeric data\n",
    "df['bins'] = pd.cut(df['col'], bins =[ 20,30,40], labels = ['young', 'mid', 'old'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random dataframe\n",
    "df2 = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 2)), \\\n",
    "                   columns=['a', 'b'])\n",
    "df2['b'][2] ='ff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datetime\n",
    "import datetime as dtt\n",
    "datum = pd.date_range('2017',freq='M', periods=12) #'D','Y'\n",
    "pd.read_csv('csv', parse_dates= ['a','b'], index_col=Date) #complises datetime from year,month,day cols, index.name\n",
    "## creates own\n",
    "y,m,d = range(2010,2021),range(1,13),range(1,8)\n",
    "date = [(i,ii,iii) for i in y for ii in m for iii in d]\n",
    "dt = pd.to_datetime(pd.DataFrame(date, columns=['year','month','day']))\n",
    "\n",
    "#datum filenevbol, str\n",
    "v = re.sub(r'\\D\\D\\D.\\D*','',filenev)\n",
    ">2018.01.02\n",
    "datum = dtt.datetime.date(dtt.datetime.strptime(v, '%Y.%m.%d'))\n",
    "print(datum)\n",
    "datum\n",
    ">2018-01-02\n",
    ">(2018,01,02) #type datetime date\n",
    "d = dt.datetime.strftime(dt.datetime.strptime(v, '%d%m%Y'), '%Y-%m-%d')\n",
    ">2018-01-02 #type str\n",
    "\n",
    "#datum lista letrehozasa ev oszlopbol es honap oszlopbol egy adatbazisban. \n",
    "#a nap konretan meg van adva\n",
    "dates = [ dtt.datetime(year=int(i[1]), month=int(i[2]), day=1) for i in data[1]]\n",
    "f['Dates']=pd.to_datetime(f['Date']) #pd direct conversion from string\n",
    "f['Year']=pd.DatetimeIndex(f['Dates']).year #extract year\n",
    "\n",
    "dtt.datetime.now().month # day,year,time,hour,minutes\n",
    "dtt.datetime.now().strftime(\"%Y-%m-%d %H:%M\") #format\n",
    "\n",
    "#text conversion to datetime\n",
    "datetext = [...]\n",
    "datetime = [dtt.striptime(i, '%b %d %Y %I:%M%p') for i in datetext] #%d nap, kiszedi a honapot/evet, atformazza\n",
    "\n",
    "# in df\n",
    "df['date'] = pd.to_datetime(df[date]) # = df['date'] = df[date].map(lambda x: dt.datetime.strptime(x, '%y%m%d').strftime('%Y/%M/%d'))\n",
    "df['date'] = df[date].map(lambda x: x.strftime('%Y/%M/%d') # reformat\n",
    "## select only some of the date items then reformat \n",
    "df['date'] = df['date'].map(lambda x: x.date()) # or .time()                         \n",
    "df['date'] = df['date'].map(lambda x: dt.datetime(x.day, x.year, x.month).strftime('%y<%B>%d'))\n",
    "df['Date'].dt.hour \n",
    "# Gouping\n",
    "## set date as index\n",
    "df.set_index(df['date'], inplace = True) # already pd.Datetime\n",
    "### turn to Datetimeinndex makes it easy to separate date/time parts\n",
    "time = pd.DatetimeIndex(df['date'])\n",
    "df.set_index(time.hour)\n",
    "### directly set the whole bunch\n",
    "df.set_index(df['date'], inplace = True)\n",
    "#resampling downscaling: means group by time units\n",
    "df.resampling('2D').mean() # get the mean ofthe two days activity\n",
    "## T/H/D/B/W/M/Q/A == min/hour/day/business days/week/month/quarter/year\n",
    "#resampling upscaling: introducesmaller timeframes than we have and fill up with some value\n",
    "df.resample('4H').ffill() # returns timeseries with 4hours frame and forwardfill missing values\n",
    "                          \n",
    "##grouping\n",
    "df.resample('M').count()\n",
    "df.groupby(pd.Grouper(freq = 'M')).count() # sligthly faster, more versitile\n",
    "df.groupby(df.index.month).count()\n",
    "                          \n",
    "# date manipulation\n",
    "datetime - datetime\n",
    "datetime + dt.timedelta(years=, month = , days= , hours= , seconds = )\n",
    "                          \n",
    "                          \n",
    "                        \n",
    "                          #codes example\n",
    "\"\"\"\n",
    "%b Month as locale’s abbreviated name(Jun)\n",
    "%B month full name\n",
    "%m month with No\n",
    "%d Day of the month as a zero-padded decimal number(1)\n",
    "%Y Year with century as a decimal number(2015)\n",
    "%y two digit year\n",
    "\n",
    "%I Hour (12-hour clock) as a zero-padded decimal number(01)\n",
    "%H 24h format\n",
    "%M Minute as a zero-padded decimal number(33)\n",
    "%p Locale’s equivalent of either AM or PM(PM)\n",
    "\n",
    "\"\"\"\n",
    "# matplotlib with plot_date()\n",
    "import matplotlib.dates as mdates\n",
    "dates = [mdates.strpdate2num(i,'%y-%m-%d') for i in list]\n",
    "\n",
    "# convert unix time to datetime\n",
    "function = np.vectorize(dt.datetime.fromtimestramp)\n",
    "dates = function(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###data exploration in dataframe\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "####################################\n",
    "####################################\n",
    "'''\n",
    "df.shape\n",
    "df.describe(include='all')\n",
    "df.columns\n",
    "df.dtypes()\n",
    "df.isnull().sum()\n",
    "df.notnull()\n",
    "df['oszlop'].value_counts() #normalized=True,dropna=False : gives proprtion instead of real number\n",
    "df['oszlop'].unique() #egyedi ertekek Seriesbe\n",
    "df.dropna().unique() #without the N/A\n",
    "df['oszlop'].size #hanyfele ertek van\n",
    "df.dtype.value_counts() #how many different col types have\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###data cleaning in dataframe\n",
    "df.columns = listofcolumns # \n",
    "df[oszlop]=df[oszlop].str.replace('mit','mire')\n",
    "#duplictes\n",
    "df.duplicated(subset=['address','area','room'], keep=False) #finding duplicates\n",
    "df.drop_duplicates(subset=df.loc[:,'price':'date'] ,inplace = True, keep= 'first' ) #slice cols, need reset_index\n",
    "df.drop(df.loc[ind['col:'].str.contains('buu') == True].index, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#missing value manipulation\n",
    "df.isnull().sum()\n",
    "df.isnull().sum().to_frame()[df.isnull().sum().to_frame()[0] != 0]\n",
    "df.mean()\n",
    "df.median()\n",
    "df.mode()\n",
    "df.fillna(df.mean())\n",
    "df.dropna(axis=0, subset=['oszlop1','oszlop2'])#1: oszlop, 0: sor subset csak sornal van erteleme\n",
    "df = df.dropna(axis = 1, how = 'all'), #if every value missing from the cols\n",
    "\n",
    "given_values = {'1.col': 'w', '2.col': 2}\n",
    "df.fillna(value=given_values) #az adott oszlop ertekeit  megdott ertkre csereli egy mentetben, oszlopspecifikusan\n",
    "df [df['oszlop'] < 0] = 0 # a negative ertekeket 0 ra csereli a teljes dataframe-ben\n",
    "df.loc[df['oszlop'] < 0, 'oszlop'] = 0 # az adott oszlopra korlatozza a cseret\n",
    "df.loc[df.index[0:10], 'c'] = 'e' = # filtering by index and columns value reset\n",
    "# missing data replace with mean\n",
    "df.loc[df['Age'].isnull(),'Age'] = np.mean(df['Age']) \n",
    "df[''].replace(['', 'NaN', None], 0, inplace = True) # replace empty cells\n",
    "\n",
    "## in arrays\n",
    "import sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(mising_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer.fit(df[:,1:3]) # works on cols 2&3\n",
    "df[:,1:3] = imputer.transform(X[:,1:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data transformation\n",
    "#bin the daily data - works only with datetime index\n",
    "bike = read.resample('d').sum()\n",
    "#adat transformacio szoveg kategoria ertekke\n",
    "df = pd.get_dummies(df, columns=['','']) \n",
    "df = pd.factorize(df['oszlop'])[0]\n",
    "from sklearn.preprocessing import LabelEncoder OneHotEncoder\n",
    "label_X = LabelEncoder()\n",
    "X[:,0] = label_X.fit_transform(X[:,0]) # factorize\n",
    "one = OneHotEncoder(categorical_features = [0]) # list of columns to change; dummies\n",
    "X = one.fit_transform(X).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#matplotlib x=datetime need to import converter\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[figure.figsize]= 8,4\n",
    "plt.style.use('ggplot') # general styles: 'seaborn', 'bmh', 'dark_background','fivethirtyeight'\n",
    "plt.title() #formating to bold\n",
    "plt.suptitle('') #give a main title to all\n",
    "plt.xticks(range(9), lista, color='') # ha listabol kulon akarjuk behivni az ertekeket\n",
    "plt.xscale(\"log\") #log scale for transformation\n",
    "#a szam megjelenites int() kell megadni. ha str() van akkor rossz lesz a sorrend\n",
    "\n",
    "#double axes on same plot.usefu to set sae colro for label and plot\n",
    "ax1.plot()\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(c='r')\n",
    "ax2.set_ylabels(c='r')\n",
    "for label in ax2.get_yticklabels():\n",
    "    label.set_color(\"red\")\n",
    "\n",
    "plt.bar(DSplotKey,DSplotValue, color='red', label = 'keyvalue', bottom = data2) #x tengely, y tengely, lista\n",
    "plt.hist(df[''], scacked=True, rwidth=0.9) #emeletes histo, for ciklussal erdemes\n",
    "plt.plot(adat, c='color', ls='-.-', lw = 1)  \n",
    "plt.stackplot(x, y1,y2,y3, c=['',''], labels = ['',''])\n",
    "plt.pie(data, labels = [], c = [], \n",
    "        startangle = 90, #degree of first slice from 12o'clock\n",
    "        shadow = True, explode = (0.1,0,0,0.2) # slice stepout. first and fourth\n",
    "        autopct = '%1.1f%%' # slice % part to the whole. autocalculated, float one decimal\n",
    "        counterclock=False,\\\n",
    "        colormap=plt.cm.gist_ncar,\\\n",
    "        fontsize=8,\\\n",
    "        wedgeprops={'linewidth':0}\n",
    "\n",
    "#scatter plot trukk for ciklusban az oszlopnevek alapjan ad egy attekintest.\n",
    "#informativabb mint a box plot\n",
    "for i in data.columns:\n",
    "    plt.scatter(data[i].value_counts().index,data[i].value_counts().values)\n",
    "    print(i)\n",
    "    plt.show()\n",
    "    \n",
    "##############################################\n",
    "# Dataframe built-in graphs based on Matplotlib\n",
    "#whiskers and box\n",
    "DS['oszlop'].plot(kind='box', subplots=True, layout=(1,1), sharex=False, sharey=False, figsize=(9,5))\n",
    "DS.boxplot() vagy DS.plot(kind='box')\n",
    "DS.boxplot('cols1', by = 'col2') #col1 num by col2 cat\n",
    "DS.plot(kind='line', color='') #color nev vagy RGB\n",
    "DS.hist('oszlop')\n",
    "DS.hist('oszlop1', 'oszlop2') # az oszlop1 az oszlop2 felbontasaban\n",
    "DS.hist(bins=10) #continuous var groupping to 10 bins\n",
    "\n",
    "#plot method use index for the x categories\n",
    "#if the categories are in mixed order, set them to index\n",
    "df.set_index(oszlop, inplace=True, drop=True)\n",
    "df.index.name='' #erase the column name\n",
    "\n",
    "#horzontal bar, nice green coloour set with fixed range\n",
    "us[oszlop].plot.barh(\n",
    "    title=\"Top Selling Genres in the USA\",\n",
    "    xlim=(0, 625),\n",
    "    colormap=plt.cm.Accent\n",
    ");\n",
    "\n",
    "# kozvetlen df vizualizacio\n",
    "## ha nem hivunk meg plt fig et elotte df.plot.hist(), ha meghivunk df.hist()\n",
    "##types\n",
    "    df.plot.area(alpha=0.4)\n",
    "    df.plot.barh\n",
    "    df.plot.density\n",
    "    df.plot.hist\n",
    "    df.plot.line\n",
    "    df.plot.scatter(x, y, c='C',cmap='coolwarm') # color by 3rd col. ; s=df['C']*100 show by dotsize, only with numeric \n",
    "    df.plot.bar(stacked=True) # retegelt oszlop\n",
    "    df.plot.box\n",
    "    df.plot.hexbin(x,y, gridsize=25,cmap='Oranges')\n",
    "    df.plot.kde\n",
    "    df.plot.pie\n",
    "## multiple graphs by features. without it it packs on top of each others\n",
    "fig = plt.figure\n",
    "ax = fig.add_subpot(111)\n",
    "df.hist(ax= ax)\n",
    "\n",
    "# if you want to add extra legend, add an empty graph \n",
    "ax1.plot([],[], label= 'High', color = 'g')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib subplots\n",
    "##################################\n",
    "##################################\n",
    "### basic manual method locl construction, calling one by one\n",
    "plt.subplot(2,1,1) #No of plots, ncols, nrows\n",
    "plt.plot()...\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot()...\n",
    "\n",
    "### ha subplot=1,1, akkor nem kell index, ha egy sik, ax[0], ha matrix ax[0,1]\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,5))\n",
    "### ax meghivas fuggvenyen belul, csak df\n",
    "df.plot.pie(ax = ax2)\n",
    "### fuggvenyen kivul, flatten()\n",
    "ax2 = df.plot.pie() #ha nem akarunk osszevont fuggvenyt\n",
    "### tulajdonsagkent, plt kozveteln\n",
    "ax2.pie() #flatten()\n",
    "ax[1].pie() #egy sorors\n",
    "ax[0,1].pie() # matrix\n",
    "### listaba kenyszeriti a matrixot\n",
    "ax1, ax2,ax3, ax4 = ax.flatten()\n",
    "\n",
    "fig.subplots_adjust(hspace=.3, wspace=.4, left=None, bottom=None, right=None, top=None)\n",
    "\n",
    "\n",
    "ax.bar(toprl['Country'] ,toprl['remain'], label = 'remain', color = '#afef22')\n",
    "ax.bar(toprl['Country'] ,toprl['leave'], label = 'leave', color = 'red', bottom=toprl['remain']) #bottom= stacked bar\n",
    "\n",
    "\n",
    "cust=df['No_of_customers'].copy().rename('') #data separation renaming to nothing so it wont be displayed\n",
    "cust.index.name='' # index denamization\n",
    "\n",
    "\n",
    "ax2.tick_params(top=\"off\", right=\"off\", left=\"off\", bottom=\"off\")\n",
    "ax2.axhline(0, color='k')\n",
    "ax2.spines[\"top\"].set_visible(False) # frame set\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "ax2.legend().set_visible(False) # remove legend\n",
    "ax.set_xticklabels(toprl['Country'], rotation = 20, va = 'center', ha = 'right', position = (0,0))\n",
    "ax.set_xlabel/set_ylabel/.set_title\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "## Object oriented invoking\n",
    "# graph a graph-ban, kulcs add_axes\n",
    "\n",
    "fig = plt.figure() # create the object\n",
    "    # main axes, populate the object with axes'\n",
    "axes1 = fig.add_axes([0.1, 0.1, 0.9, 0.9])  # left, bottom, width, height % of the canvas (range 0 to 1)\n",
    "    # inner axes\n",
    "axes2 = fig.add_axes([0.2, 0.2, 0.4, 0.4])\n",
    "    # main figure\n",
    "axes1.plot(x, y, 'r') # 'r' = red line\n",
    "axes1.set_xlabel/set_ylabel/.set_title()\n",
    "    # inner figure\n",
    "axes2.plot(x2,y2, 'g')   # 'g' = green line\n",
    "axes2.set_xlabel/set_ylabel/.set_title\n",
    "plt.show()\n",
    "\n",
    "##################\n",
    "## Subplots alkalmazas egyszerusiti az axis letrehozast row/col rendszerben. \n",
    "## automatikusan hivja az add_axes methodot a row/col megfeleloen\n",
    "\n",
    "# subplots hivatkozas kooordinatakkal in grid less then 2 doesn't work\n",
    "ncols, nrows = 3, 3\n",
    "fig, axes = plt.subplots(nrows, ncols)\n",
    "for m in range(nrows):\n",
    "    for n in range(ncols):\n",
    "        axes[m, n].scatter(X[:,m+n], y) #from np array\n",
    "        axes[m, n].set_xticks([])\n",
    "        axes[m, n].set_yticks([])\n",
    "        axes[m, n].text(0.5, 0.5, \"axes[{}, {}]\".format(m, n),\n",
    "                        horizontalalignment='center') # text a graphra\n",
    "\n",
    "#################\n",
    "# subplots in customized shape\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    # Let's remove all labels  on the axes, just a trick nt necessary as the implaning goes in loop. \n",
    "    ##if implenting is induvidual, useful shorthand\n",
    "def clear_ticklabels(ax):\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    # prepare the layout first set: grid, second set: start position, span to right and down\n",
    "ax0 = plt.subplot2grid((3, 3), (0, 0), colspan = 2)\n",
    "ax1 = plt.subplot2grid((3, 3), (1, 0), colspan = 2)\n",
    "ax2 = plt.subplot2grid((3, 3), (2, 0), colspan = 2)\n",
    "ax3 = plt.subplot2grid((3, 3), (0, 2), rowspan=3)\n",
    "axes = (ax0, ax1, ax2, ax3) # put them in a list to easy call\n",
    "\n",
    "for i in range(0,len(axes)):\n",
    "    axes[i].scatter(X[:,i], y)\n",
    "    axes[i].set_title(file[0][i])\n",
    "    axes[i].set_yticks([])\n",
    "    axes[i].annotate('adj-R: %.3f' % (r), xy=(1,max(y)*0.8) ) # relative annotalas az y-hoz\n",
    "\n",
    "    # Add all sublots\n",
    "    #[ax.text(0.5, 0.5, \"ax{}\".format(n), horizontalalignment='center') for n, ax in enumerate(axes)]\n",
    "# Clear labels on axes, optional see above\n",
    "[clear_ticklabels(ax) for ax in axes]\n",
    "plt.legend(loc = (0.1,0.1), borderaxespad=0.) # bbox_to_anchor = location, shaded border\n",
    "plt.show()\n",
    "\n",
    "######################\n",
    "## subplot letrehozas menet kozben. rugalmasabb megoldas, hivatkozas sorszammal (i)\n",
    "# subplots automata\n",
    "fig = plt.figure( figsize = (15, 5))\n",
    "for i in range(1, len(df.column)):\n",
    "    ax = fig.add_subplot(1, 3, i)   # (rows amount, columns amount, subplot number)\n",
    "    ax.plot(x, y**(i+1), 'r')\n",
    "plt.tight_layout() # beszoritja a canvasba\n",
    "\n",
    "#subplot automata in row    \n",
    "fig = plt.figure( figsize = (15, 5))\n",
    "for i in range(0,X.shape[1]):\n",
    "    ax = fig.add_subplot(1,X.shape[1], i+1) # (rows amount, columns amount, subplot number)\n",
    "    ax.scatter(X[:,i], y)\n",
    "    ax.set_title(file[0][i]) #if you have categoricat value, it is better to see even if the numbers are packed\n",
    "    ax.set_yticks([])  #no value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib dates\n",
    "\n",
    "##this dates linux dates, works only with plt.plot_dates()\n",
    "### this is the pattern of the original numbers\n",
    "import matplotlib.dates as mdates\n",
    "dates = mdates.strpdate2num('%y%m%d')('150120') #or lists of numbers\n",
    "### from self calling functions like loadtxt\n",
    "dates = np.loadtxt(array, delimiter= , unpack = True, convenrters = { 0: mdates.bytespdate2num('original fomat %Y%m%d')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib geomaps\n",
    "import folium\n",
    "phone_map = folium.Map()\n",
    "\n",
    "# Marker location list of dicts\n",
    "companies = [\n",
    "    {'loc': [37.4970,  127.0266], 'label': 'Sams'},\n",
    "    {'loc': [37.3318, -122.0311], 'label': 'Apple'},\n",
    "\n",
    "# Adding markers to the map\n",
    "for company in companies:\n",
    "    marker = folium.Marker(location=company['loc'], popup=company['label'])\n",
    "    marker.add_to(phone_map)\n",
    "\n",
    "# The last object in the cell always gets shown in the notebook\n",
    "phone_map\n",
    "    \n",
    "############################\n",
    "############################\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "# A basic map\n",
    "m = Basemap(projection='lcc', resolution='c',\n",
    "            width=3E6, height=3E6, #slice of the map (China)\n",
    "            lat_0=30, lon_0=110)\n",
    "m.etopo(scale=0.5, alpha=0.5)\n",
    "cities = [[39.1422,117.1766,'Tianjin', 3.54],...] #marker excample: loc, text,size\n",
    "# Map (long, lat) to (x, y) for plotting\n",
    "for i in cities:\n",
    "    x, y = m(i[1], i[0])\n",
    "    plt.plot(x, y, 'ok', markersize= i[3]*2)\n",
    "    plt.text(x*(1+i[3]/200), y, i[2], fontsize=12, ha = 'left', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib customization\n",
    "#the curve\n",
    "ax.plot(x,y, color=\"purple\", lw=1, ls='-', /\n",
    "        marker='s', markersize=8, markerfacecolor=\"yellow\", /\n",
    "        markeredgewidth=3, markeredgecolor=\"green\")\n",
    "# axis range limit\n",
    "ax.set_ylim(value) / ..xlim() #custom value\n",
    "ax.axis('tight') # tight fit to th curve\n",
    "\n",
    "#change axis for a particular graph\n",
    "plt.setp(ax, xticks=, xtickslabel= , yticks=) #setting general values\n",
    "plt.sca(ax[1,2]) #set individual statements below refers to the plot\n",
    "plt.xticks([list of value points], [list of labels], color=, rotation=)\n",
    "https://stackoverflow.com/questions/19626530/python-xticks-in-subplots\n",
    "\n",
    "#annotalas df ertekkel, kiegeszito adatmegjelenites lds: chinook dataquest\n",
    "for i, value in enumarate(df.index):\n",
    "    x_pos = df.loc[value, ertekoszlop]\n",
    "    #ha float erteket akarunk kiirni szazalekkent\n",
    "    display = df.loc[value, displayoszlop].astype('int').astype('str') + \"%\"\n",
    "    plt.annotate(display, (x_pos, i))\n",
    "    ax[i-1].text(8,0,'aR2: %.3f' % stat) # coordinata, szoveg\n",
    "    ax[i-1].annotate(str(), xy=[max_year, gdp_value], xytext=[7, -2], textcoords='offset points') # ponthoz igazitas\n",
    "    \n",
    "# custom legedns\n",
    "import matplotlib.patches as mpatches\n",
    "red_patch = mpatches.Patch(color='Green', label='Remain',)\n",
    "g_patch = mpatches.Patch(color='Red', label = 'Churn')\n",
    "plt.legend(handles=[red_patch, g_patch])\n",
    "\n",
    "# add grid\n",
    "ax1.grid(True, color = , linewidth =)\n",
    "#add horuzontal line\n",
    "ax.axhline(hight value, color, linewidth=)\n",
    "\n",
    "ax.spines['top'].set_visible(False) #remove edges\n",
    "                .set_color('g') \n",
    "ax.tick_params(axis='x', color = ) # color of the axis notes\n",
    "\n",
    "# fill up the place between\n",
    "plt.fill_betweem(x, y, z) # the axis or reference of filling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c2ee87ec7ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m \u001b[0;31m#extension of matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjointplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkdeplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshade\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Seaborn visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #extension of matplotlib\n",
    " #univariate plots\n",
    "sns.distplot(tips['bill'],kde=False,bins=30)\n",
    "sns.jointplot(x='bill',y='tip',data=tips,kind='scatter') # kind:hex, reg, + distribution bins\n",
    "sns.pairplot(tips,hue='sex',palette='coolwarm') # ossze continuous var pairwise\n",
    "sns.rugplot(tips['bill']) # only ticks for distribution (like linecode), very simple\n",
    "sns.kdeplot(x,y, shade=True)\n",
    "sns.regplot() #linear reg + confidence intervals\n",
    "\n",
    "    #categorical\n",
    "sns.barplot(x='sex',y='bill',data=df, estimator=np.std) \n",
    "sns.countplot(x='sex',data=df) # simple, only counts\n",
    "sns.boxplot(data=df,palette='rainbow',orient='h') # horizontal, every oszlop\n",
    "sns.boxplot(x=\"day\", y=\"bill\", hue=\"sex\",data=df, palette=\"coolwarm\") # pairwise, hue: 3rd paramenter(binal) colorised\n",
    "sns.violinplot(x=\"day\", y=\"bill\", data=df,hue='sex',split=True, palette='Set1') # pslit: left/right hue\n",
    "sns.stripplot(x=\"day\", y=\"total_bill\", data=tips,jitter=True,hue='sex',palette='Set1') # jitter: points in line but wider\n",
    "sns.swarmplot(x=\"day\", y=\"total_bill\",hue='sex',data=tips, palette=\"Set1\", split=True) # all points in distribution\n",
    "sns.factorplot(x='sex',y='total_bill',data=tips,kind='bar') # like bar\n",
    "\n",
    "    # correltations\n",
    "sns.heatmap(tips.corr(),cmap='coolwarm',annot=True, standard_scale=1) \n",
    "'''create a df with the pairs, groupby with the two columns of interest. crates a multiindex df. use unstack to make new df with\n",
    "the pairs. use only the df name to call. little coloured sports ,cat/cat. orderly dispay'''\n",
    "sns.clustermap(df,cmap='coolwarm',standard_scale=1) # similarity reorganisation of the data, std_scale: normalise\n",
    "sns.lmplot(x,y, hue=categories) # linear regression with CI\n",
    "sns.lmplot('index', 'col', data =  m.reset_index()) # it cant use index as regression value. need to be reset. sometimes after groupby count\n",
    "    \n",
    "#FacetGrid\n",
    "kws = dict(s=25, linewidth=0.5, edgecolor='black') #ez a set a pottyoket allitja be\n",
    "g = sns.FacetGrid(data, row='oszlop1', col='oszlop2', hue='oszlop1', kws)\n",
    "g = g.map(plt.scatter, 'oszlop1', 'oszlop2', **kws) #egyeb plottal is dolgozik\n",
    "\n",
    "#subplots with sns\n",
    "fig, ax ...\n",
    "    #directly\n",
    "ax5 = sns.countplot(df['IsActiveMember'], color= 'red', ax = ax[1,2]) #two graf one place\n",
    "ax6 = sns.countplot(df['Exited'], ax = ax[1,2])\n",
    "ax6.set_xticklabels(['Exites', 'Stayed']) # change labels\n",
    "\n",
    "    #flatten\n",
    "ax1, ax2 = ax.flatten() #puts them in var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive graphs Plotly, Cufflinks works with df\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "print(__version__) # requires version >= 1.9.0\n",
    "import cufflinks as cf\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modellezes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#correlation check\n",
    "df.corr()\n",
    "import scipy.stats as sp\n",
    "p = df.pearsonr(col1,col2); p[0], p[1] #correltaion, P-value\n",
    "s = df.spearmanr(col1,col2); s[0]  s[1]\n",
    "k = df.kendalltau(col1,col2); k[0] k[1]\n",
    "\n",
    "#homoscedescency test, equality of variance, if sample size is different\n",
    "from scipy.stats import Levene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adat felosztas az algoritmus elesitesere\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.5, random_state = 42)\n",
    "# size a megoszlasi arany a train es a test kozott\n",
    "print('X train shape: %s' % str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling mindig a felosztas utan kell, mert a kevesebb adat jobb scale-t ad.\n",
    "##categorical valtozokat is erdemes scalelni a jobb predikcio miatt. igy azonban informaciot veszthet a modell.\n",
    "##feature scaling noveli a sebesseget, meg akkor is ha nem euclideszi tavolsagon alapul az algoritmus\n",
    "##classification nem kell an indep var scalelni, de regressional igen\n",
    "## szamos algoritum tartamazza automatikusan a feature scaling-et, nem kell kulon alkalmazni\n",
    "# az mintak a sorok a feature az oszlop. ha mashogy van df.T - transpose\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train) # az elso lepesben fit-elni kell, ha ujra hasznaljuk mar nem kell\n",
    "X_test = sc_x.transform(X_test)\n",
    "\n",
    "# real value retrival after prediction\n",
    "y_pred = sc_y.inverse_trainsform(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simple/ multiple linear regression\n",
    "#X_ matrix, y_ vector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train,y_train)\n",
    "lin.predict(??) #x_test, querry value\n",
    "lin.coef_\n",
    "lin.intercept_\n",
    "\n",
    "#polynomial regression simple reg. for non-linear set\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pol = PolynomialFeatures(degree = 2)\n",
    "Xpol = pol.fit_transform(X)\n",
    "reg = lin.fit(Xpol,y)\n",
    "ypred = reg.predict(Xpol,y)\n",
    "\n",
    "#SRM sv regression; scaling kell!\n",
    "from slearn.srm import SVR\n",
    "reg = SVR() # hyperbolic data kernel='rbf', lieanr data kernel='linear'\n",
    "Xreg = reg.fit(X,y)\n",
    "ypred = Xreg.predict(X)\n",
    "y_pred = sc_y.inverse_trainsform(ypred) # valos value\n",
    "ypred = Xreg.predict(std_x.fit_transform(np.array([[50]])))    #double brackets to pretend an (1,1) array\n",
    "      \n",
    "# Decision tree regression - non or linear, no-continuous modell\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(random_state = 0)\n",
    "reg = df.fit(X,y)\n",
    "ypred = reg.predict(X)\n",
    "      \n",
    "# Random Forest - avearge of DTs\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rt = RandomForestRegressor(n_estimator = 10)\n",
    "\n",
    "#KNN regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(algorithm='brute', metric = 'euclidean', n_neighbors=5)\n",
    "\n",
    "#Evaluation\n",
    "## R2 score\n",
    "lin.score(X,y) # R2\n",
    "lin.coef_\n",
    "lin.intercept_\n",
    "X_stat = X[:, [0,1,2,3...]]\n",
    "\n",
    " ### add extra '1' for the coefficient\n",
    "Xopt = np.append(np.ones(len(X)).reshape(-1,1), X, axis= 1 ) # or np.hstack\n",
    "from statsmodels.formula.api import OLS\n",
    "stat = OLS(y, X_stat).fit().summary()\n",
    "print(OLS(y,X_opt).fit().pvalues, \\\n",
    "OLS(y,X_opt).fit().rsquared, \\\n",
    "OLS(y,X_opt).fit().rsquared_adj\n",
    "      \n",
    "##MAE MSE, RMSE (root) mean absolute/square error\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "Reg_mae = mae(test,pred)  # if the numbers are small, mean average of errors\n",
    "Reg_mse = mse(test,pred) # mean square of errors\n",
    "Reg_rmse = reg_mse ** 0.5 # root of mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting threshold with probabilistikus modelekkel\n",
    "yhat = (classi.predict_proba(Xtest)[:1] >= 0.3).astype(bool)\n",
    "# linear\n",
    "    ## logistic regrssion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression()\n",
    "log.predict_proba(Xtest) # getting the curve probabilities\n",
    "np.sort(yhat[0]) #for plt.plot\n",
    "        ### var selection & evaluation\n",
    "from statsmodels.formula.api import Logit\n",
    "Xcol = np.hstack([np.ones(len(X), dtype=float).reshape(-1,1), X]) # adding X0 manually\n",
    "Logit(y,Xcol).fit().summary()\n",
    "result.params # coefficients\n",
    "result.conf_int() # conf intervals\n",
    "\n",
    "log.score(Xtrain,ytrain)\n",
    "log.coef_\n",
    "log.intercept_\n",
    "\n",
    "    ##SVC\n",
    "    ### pseudo - probabilistic\n",
    "from sklearn.svm import SVC\n",
    "classi = SVC(kernel = 'linear', prabability=True)\n",
    "\n",
    "    ## Kernel SVC\n",
    "classi = SVC(kernel='rbg', probability = True) # rgb, sigmoid, polinomial\n",
    "\n",
    "# non-linear\n",
    "\n",
    "    ## KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clasi = KNeighborsClassifier(n_neighbors= 5 )\n",
    "    \n",
    "    ##random forest\n",
    "    ### pseudo- probabilistikus\n",
    "    \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randForest = RandomForestClassifier(n_estimators=25, min_samples_split= 25, max_depth= 7,\n",
    "                                    max_features= 1)\n",
    "randForest.fit(X_train, y_train) #  classification rule\n",
    "y_pred_RF = randForest.predict(X_test) # checking the classification rule\n",
    "\n",
    "    ## Naive Bayes\n",
    "    ### probabilistic\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classi = gnb()\n",
    "\n",
    "    # ertekeles\n",
    "from sklearn.metrics import accuracy_score\n",
    "randForestScore = accuracy_score(y_test, y_pred_RF) # osszehasonlito pontertek\n",
    "pd.crosstab(y_test, y_pred_RF,rownames=['Actual'], colnames= ['Predicted']) # contigency matrix valos ertekekkel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    #evaluation confusion matrix\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "cm(ytest,yhat)\n",
    "        #or\n",
    "pd.crosstab(y_test, y_pred_RF,rownames=['Actual'], colnames= ['Predicted']) # contigency matrix valos ertekekkel\n",
    "\n",
    "dick = {'Acc' : (cmsc[0,0]+cmsc[1,1])/cmsc.sum(),\n",
    "         'Err:' : (cmsc[0,1]+cmsc[1,0])/cmsc.sum(),\n",
    "         'Sens:' : cmsc[0,0]/cmsc[0,:].sum(),\n",
    "         'Spec:' : cmsc[1,1]/cmsc[1,:].sum(),\n",
    "         'Recall:' : cmsc[1,1]/cmsc[:,1].sum(),\n",
    "         'Prec:' : cmsc[0,0]/cmsc[:,0].sum()\n",
    "         }\n",
    "\n",
    "# var selection & evaluation\n",
    "from statsmodels.formula.api import Logit\n",
    "Xcol = np.hstack([np.ones(len(X), dtype=float).reshape(-1,1), X]) # adding X0 manually\n",
    "Logit(y,Xcol).fit().summary()\n",
    "result.params # coefficients\n",
    "result.conf_int() # conf intervals\n",
    "\n",
    "\n",
    "log.score(Xtrain,ytrain)\n",
    "log.coef_\n",
    "log.intercept_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation od probability classifiers (logaritmic regression, SVM classifier, RF classifer)\n",
    "\n",
    "# ROC curve\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score #auc general, roc_auc for roc mindegy\n",
    "fpr, tpr, treshold = roc_curve(ytest, yprob) # false positive rate, true positive rate \n",
    "auc(fpr,tpr) vs. roc_auc_score(ytest, yprob) #teruletertek, mas a kiindulasi adat de az eredmeny ua.\n",
    "plt.plot([0,1],[0,1], label='Chance', linestyle = '--', lw= 2, color='red')\n",
    "plt.plot(fpr,tpr, label'ROC auc= %.3f%%' % roc_auc_score)\n",
    "\n",
    "# CAP curve\n",
    "# only within test not the whole set of data!\n",
    "## with true numbers\n",
    "total = len(ytest)\n",
    "class_1_count = np.sum(ytest)\n",
    "class_0_count = total - class_1_count\n",
    "cap = [l for _,l in sorted(zip(y_prob[:,1],ytest), reverse= True)] # order by prob, get the tests\n",
    "cap = np.append([0], np.cumsum(cap))\n",
    "\n",
    "plt.plot([0, total], [0, class_1_count], label = 'Random Model')\n",
    "plt.plot([0, class_1_count, total], [0, class_1_count, class_1_count], label = 'Perfect Model')\n",
    "plt.plot(range(0,len(cap)), cap, label = 'Normal dependents')\n",
    "# 50% result\n",
    "plt.plot([int(len(cap)/2), int(len(cap)/2)], [0, cap[int(len(cap)/2)]], linestyle = '--', c = 'green')\n",
    "plt.plot([int(len(cap)/2), 0], [cap[int(len(cap)/2)], cap[int(len(cap)/2)]], linestyle = '--', c = 'green')\n",
    "plt.text(0, cap[int(len(cap)/2)]+10, cap[int(len(cap)/2)])\n",
    "\n",
    "## with precentage\n",
    "cap_ = cap*100/ytest.sum()\n",
    "pos_ = ytest.sum()*100/ytest.size \n",
    "\n",
    "plt.plot(range(len(cap_)), cap_, label = 'CAP')\n",
    "plt.plot([0,100], [0,100], label = 'Chance')\n",
    "plt.plot([0, pos_, 100],[0, 100, 100 ], label = 'BEST')\n",
    "plt.plot([int(ytest.size/2), int(ytest.size/2)],[0, cap_[int(ytest.size/2)]], linestyle = '--', c= 'r')\n",
    "plt.plot([int(ytest.size/2), 0] , [cap_[int(ytest.size/2)], cap_[int(ytest.size/2)]], linestyle = '--' , c= 'r')\n",
    "plt.annotate('{:.2f}'.format(cap_[int(ytest.size/2)]), xy=[0,cap_[int(ytest.size/2)] + 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K- means\n",
    "from sklearn.cluster import KMeans\n",
    " ## Determine the No of clusters by WCSS\n",
    "wcss = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_custers = i)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss) #check for line breaks\n",
    "\n",
    " ## the clusters\n",
    "kmeans = KMeans(n_clusters = 5)\n",
    "y_clust = kmeans.fit_predict(X) # X array or df\n",
    "centroids = kmeans.cluster_centers_ # array\n",
    "\n",
    " ## plot if only two features\n",
    "    ###quick way\n",
    "plt.scatter(X[:,0], x[:,1], color = colorarray[y_cluster])\n",
    "    ###detailed\n",
    "colorlist = []\n",
    "interpretation = ['target', 'looser' ...]\n",
    "for i in range(4):\n",
    "    plt.scatter(X[y_clust == i, 0], X[y_cust == i, 1], c = colorlist[i], label = interpretation[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kfold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors= 18, algorithm='brute') # create model\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state= 1) # Kfold setup\n",
    "#for regression\n",
    "score = cross_val_score(knn, X, y , cv = kf, scoring='neg_mean_squared_error' )\n",
    "RMSE = np.sqrt(np.absolute(score))\n",
    "result = np.mean(RMSE)\n",
    "#for other models\n",
    "#https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adat exportalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adat exportalas DataFrame-bol\n",
    "import csv\n",
    "df.to_csv('filename.csv', sep= '\\t', index= True) # megtartja az index-t, False elveti az index-t\n",
    "\n",
    "#export to excel\n",
    "df.to_excel('filename', sheet_name= 'sheetname', index=False, startrow=1,startcol=2) #position where to put the df\n",
    "#export multiple df to separate excel sheets\n",
    "with pd.ExcelWriter('filename') as writer:\n",
    "    df1.to_excel(writer, sheetname='')\n",
    "    df2.to_excel(writer, sheetname='')\n",
    "#append data to excel sheet\n",
    "from openpyxl import load_workbook\n",
    "book = load_workbook('movie.xlsx')\n",
    "writer = pd.ExcelWriter('movie.xlsx', engine='openpyxl') \n",
    "writer.book = book\n",
    "writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "\n",
    "df.to_excel(writer, \"originalsheet\", header=False, startrow=3)\n",
    "\n",
    "writer.save()\n",
    "    \n",
    "#export plot \n",
    "plt.savefig('filename', transparent=True, bbox_inches='tight', dpi=120)\n",
    "\n",
    "#export to txt\n",
    "##the whole thing is one big quote between \"\". not useful for dic\n",
    "with open('file.txt', 'w') as f:\n",
    "    f.write('')\n",
    "    f.write(str(var))\n",
    "    \n",
    "f = open('test.txt', 'r').read()\n",
    "\n",
    "#https://stackoverflow.com/questions/36059194/what-is-the-difference-between-json-dump-and-json-dumps-in-python\n",
    "#export to json\n",
    "import json\n",
    "dick = {}\n",
    "with open('test2.json', 'w') as js:\n",
    "    json.dump(dick, js)\n",
    "    #or\n",
    "    dick = json.load(js)\n",
    "dump = json.load(open('test2.json', 'r'))\n",
    "\n",
    "#export json a string, use for printing\n",
    "js = json.dumps(dick)\n",
    "with open('testdumps.json', 'w') as f:\n",
    "    f.write(js)\n",
    "dumps = json.loads(open('testdumps.json','r').read())\n",
    "\n",
    "\n",
    "import pickle\n",
    "dict = {}\n",
    "f = open(\"file.pkl\",\"wb\")\n",
    "pickle.dump(dict,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sql connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "#making connection , if no db exist creats it\n",
    "conn = sql.connect('jobs.db') \n",
    "#creating instance \n",
    "coursor = conn.cursor()\n",
    "coursor.table_names() #sqlalchemy\n",
    "#make query as text var\n",
    "query = 'select Major from recent_grads'\n",
    "#exec\n",
    "coursor.execute(query)\n",
    "#return result as tuple in var\n",
    "result = coursor.fetchall() // fetchone() // fetchmany(4)\n",
    "print (result[:2])\n",
    "#closeing the connection\n",
    "conn.close()\n",
    "#sqlite3 module restrict access to one instance at the moment so close it to allow others to connect too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to POSTgreSQL\n",
    "import psycopg2 as psy\n",
    "conn = psy.connect(dbname = 'chinook', user = 'anyad')\n",
    "conn.autocommit() # if you want immediate action\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(q)\n",
    "cursor.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#direct to pandas will keEp the intended layout\n",
    "q1r = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context management\n",
    "#steps to function\n",
    "def run_query(q):\n",
    "    with sqlite3.connect('chinook.db') as conn:\n",
    "        return pd.read_sql(q, conn)\n",
    "#used when no return needed like making a view\n",
    "def run_command(c):\n",
    "    with sqlite3.connect('chinook.db') as conn:\n",
    "        conn.isolation_level = None\n",
    "        conn.execute(c)\n",
    "        \n",
    "#for recurent queries a fixed function can be created\n",
    "def show_tables():\n",
    "    q = '''\n",
    "    SELECT\n",
    "        name,\n",
    "        type\n",
    "    FROM sqlite_master\n",
    "    WHERE type IN (\"table\",\"view\");\n",
    "    '''\n",
    "    return run_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-0221270f7c0b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-0221270f7c0b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    (DATETIME('now') - e.birthdate) AS Age\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#datum kezeles, age counting sqlite\n",
    "(DATETIME('now') - e.birthdate) AS Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(requests.get(url, verify=False), 'lxml') # lxml = html.parsr\n",
    "soup.prettify()  # tidy html printout\n",
    "soup.title.text #call tag as direct attribute, text\n",
    "soup.get_text() # complete text\n",
    "soup.find('div', attrs={'class':'classname'}).prettify() # need print to show the nicely\n",
    "soup.find('div', attrs={'class':'classname'})['attrname'] #get an attribute value \n",
    "soup.findAll('div', ['attr1','attr4']) #tag madatory, search among list of attrs values\n",
    "soup.find('a')[1]['href'].split('/')[-1] #last bit of the url\n",
    "#multiple instances:\n",
    "lista = soup.find('div', 'class':'name')\n",
    "for l in lista:\n",
    "    print (l.attrs['attrname'])\n",
    " #or\n",
    "    #only http, is not None to avoid error from empty href\n",
    "links = [i.get('href') for i in soup.find_all('a') if iget('href') is not None if i.get('href')[:7] == 'http://'] \n",
    "\n",
    "\n",
    "#put the in dictionary to move all to df\n",
    "#list comprehension, wildcard\n",
    "import re\n",
    "data['nev']=[a.attrs['attr_name'] for a in soup.find_all('div', attrs={'id': re.compile('yepp*')})]\n",
    "\n",
    "find(attrs).next #jumps to the next tag next.next.next .previous\n",
    "soup.find_all('div')[4].text # a talatok indexlehetok is\n",
    "soup.select('')[1].text #find_all helyett selectors: #1 : id, semmi:html, .class:class\n",
    "import requests\n",
    "r = requests.get(url, verify = False)\n",
    "parse = BeautifullSoup(r.content, 'html-parser')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
